{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import sys \n",
    "sys.path.insert(1, os.path.dirname(os.getcwd()))\n",
    "from functions import *\n",
    "import numpy as np \n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "train_size, val_size = 80, 40\n",
    "look_back, look_fwd = 10, 10\n",
    "batch_size = 8\n",
    "nodes, kernel_size = 128, 1\n",
    "activation = 'tanh'\n",
    "\n",
    "autoencoder = tf.keras.saving.load_model('cae.keras') \n",
    "\n",
    "const_dict = load_constants()\n",
    "Uf, P, T_h, T_0, Pr, Ra = get_model_constants(const_dict)\n",
    "\n",
    "train_size, val_size = 2000, 500\n",
    "data_train, data_val, x, z, _ = load_data(train_size, val_size, Uf, P, T_h, T_0)\n",
    "dx_np, dz_np, dt_np = get_grads(x, z, const_dict, Uf)\n",
    "\n",
    "dx = tf.constant(dx_np, tf.float32)\n",
    "dz = tf.constant(dz_np, tf.float32)\n",
    "dt = tf.constant(np.array(dt_np).reshape(1,), tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62353cc0",
   "metadata": {},
   "source": [
    "# CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53829ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CONTEXT BUILDER\n",
    "def get_context_builder(hidden_size, kernel_size):\n",
    "  inputs = tf.keras.layers.Input(shape=(None, 16, 16, 64), name='Inputs')\n",
    "  _, h1, c1 = ConvLSTM2D(hidden_size, kernel_size, return_sequences=True, return_state=True, padding='same', name='ConvLSTM_CB')(inputs)\n",
    "  return tf.keras.Model(inputs, [h1, c1], name='ContextBuilder_Model')\n",
    "\n",
    "# CREATE SEQUENCE GENERATOR\n",
    "@register_keras_serializable()\n",
    "class SequenceGenerator(tf.keras.Model):\n",
    "  def __init__(self, hidden_size, kernel_size, out_size):\n",
    "    super(SequenceGenerator, self).__init__(name='SequenceGenerator_Model')\n",
    "    self.hidden_size = hidden_size\n",
    "    self.kernel_size = kernel_size\n",
    "    self.out_size = out_size\n",
    "    self.rnn = ConvLSTM2D(hidden_size, kernel_size, return_state=True, padding='same', name='ConvLSTM_SG')\n",
    "    self.conv = Conv2D(64, kernel_size=3, padding='same', name='Conv2D') \n",
    "    self.norm = LayerNormalization(name='Norm')\n",
    "    self.act = LeakyReLU(0.2, name='ReLU')\n",
    "    self.expand_dim = Lambda(lambda x: tf.expand_dims(x, axis=1))\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    initial_input_shape = input_shape[0]\n",
    "    h_shape = c_shape = (1, 16, 16, self.hidden_size)\n",
    "\n",
    "    dummy_input = tf.zeros(initial_input_shape)\n",
    "    dummy_h = tf.zeros(h_shape)\n",
    "    dummy_c = tf.zeros(c_shape)\n",
    "\n",
    "    dec_o, _, _ = self.rnn(dummy_input, initial_state=[dummy_h, dummy_c])\n",
    "    _ = self.act(self.norm(self.conv(dec_o)))\n",
    "    super().build(input_shape)\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    initial_input, h, c, targets, autoreg_prob = inputs\n",
    "    T = tf.shape(targets)[1]\n",
    "    t_switch = tf.cast(T, tf.float32) * autoreg_prob\n",
    "    outputs = tf.TensorArray(dtype=tf.float32, size=T)\n",
    "    input_at_t = initial_input\n",
    "      \n",
    "    def cond_autoreg(t, input_at_t, h, c, outputs):\n",
    "      return tf.cast(t, tf.float32) < t_switch\n",
    "      \n",
    "    def body_autoreg(t, input_at_t, h, c, outputs):\n",
    "      dec_o, h, c = self.rnn(input_at_t, initial_state=[h, c])\n",
    "      output = self.act(self.norm(self.conv(dec_o)))\n",
    "      outputs = outputs.write(t, output)\n",
    "      input_at_t = self.expand_dim(output)\n",
    "      return t + 1, input_at_t, h, c, outputs\n",
    "      \n",
    "    def cond_teacher(t, input_at_t, h, c, outputs):\n",
    "      return tf.cast(t, tf.float32) < tf.cast(T, tf.float32)\n",
    "\n",
    "    def body_teacher(t, input_at_t, h, c, outputs):\n",
    "      dec_o, h, c = self.rnn(input_at_t, initial_state=[h, c])\n",
    "      output = self.act(self.norm(self.conv(dec_o)))\n",
    "      outputs = outputs.write(t, output)\n",
    "      input_at_t = targets[:, t:t+1]\n",
    "      return t + 1, input_at_t, h, c, outputs\n",
    "\n",
    "    t = tf.constant(0)\n",
    "    shape_invs = [t.get_shape(), tf.TensorShape([None, None, 16, 16, self.out_size]), tf.TensorShape([None, 16, 16, 128]), tf.TensorShape([None, 16, 16, 128]), tf.TensorShape(None)]\n",
    "    t, input_at_t, h, c, outputs = tf.while_loop(cond_autoreg, body_autoreg, loop_vars=[t, input_at_t, h, c, outputs], shape_invariants=shape_invs)\n",
    "    t, input_at_t, h, c, outputs = tf.while_loop(cond_teacher, body_teacher, loop_vars=[t, input_at_t, h, c, outputs], shape_invariants=shape_invs)\n",
    "    return tf.transpose(outputs.stack(), perm=[1,0,2,3,4])  \n",
    "\n",
    "  def get_config(self):    \n",
    "    config = super().get_config()\n",
    "    config.update({\"hidden_size\": self.hidden_size, \"kernel_size\": self.kernel_size, 'out_size':self.out_size})\n",
    "    return config\n",
    "    \n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)\n",
    "  \n",
    "  \n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "ae_encoder = build_ae_encoder(autoencoder)\n",
    "context_builder = get_context_builder(hidden_size=nodes, kernel_size=kernel_size)\n",
    "sequence_generator = SequenceGenerator(hidden_size=nodes, kernel_size=kernel_size, out_size=64)\n",
    "ae_decoder = build_ae_decoder(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7466e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "data_train, data_val, x, z = load_lstm_data(train_size, val_size, look_back, look_fwd, Uf, P, T_h, T_0, seqs_train=16, seqs_val=4, autoencoder=autoencoder)               \n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[1,look_fwd,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[1,look_fwd,256,256,4], dtype=tf.float32)])\n",
    "def loss_fn(U_true, U_pred): \n",
    "    loss_data = tf.reduce_mean(tf.math.square(U_pred-U_true), axis=[0,1,2,3])  \n",
    "    Ld_u = loss_data[0]\n",
    "    Ld_w = loss_data[1]\n",
    "    Ld_p = loss_data[2]\n",
    "    Ld_T = loss_data[3]\n",
    "    return Ld_u, Ld_w, Ld_p, Ld_T\n",
    "\n",
    "\n",
    "# VALIDATION STEP HELPER FUNCTION TO KEEP TRACK OF VALIDATION LOSS\n",
    "low_dims = 256 // (2**4)  \n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[1, look_back, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                            tf.TensorSpec(shape=[1, look_fwd, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                            tf.TensorSpec(shape=[1, look_fwd, 256, 256, 4], dtype=tf.float32),\n",
    "                            tf.TensorSpec(shape=[], dtype=tf.float32)])\n",
    "def val_step(x_batch, x_dec, U_batch, autoreg_prob):\n",
    "    h, c = context_builder(x_batch, training=False)\n",
    "    x = sequence_generator((x_batch[:,-1:], h, c, x_dec, autoreg_prob), training=False)\n",
    "    U_pred = ae_decoder(x, training=False)\n",
    "    Ld_u, Ld_w, Ld_p, Ld_T = loss_fn(U_batch, U_pred) \n",
    "    return tf.stack([Ld_u, Ld_w, Ld_p, Ld_T], axis=0)\n",
    "\n",
    "# HELPER FUNCTION FOR TRAIN STEP FOR EACH BATCH\n",
    "## Batch size of 1, gradients are accumulated to simulate larger batch size due to memory constraints\n",
    "input_signature_train = [tf.TensorSpec(shape=[1, look_back, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[1, look_fwd, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[1, look_fwd, 256, 256, 4], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[4], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[], dtype=tf.float32)]\n",
    "\n",
    " \n",
    "@tf.function(input_signature=input_signature_train)\n",
    "def train_step(x_batch, x_dec, U_batch, l_dwa, autoreg_prob):\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    h, c = context_builder(x_batch, training=True)\n",
    "    x = sequence_generator((x_batch[:,-1:], h, c, x_dec, autoreg_prob), training=True)\n",
    "    U_pred = ae_decoder(x, training=False)\n",
    "    \n",
    "    Ld_u, Ld_w, Ld_p, Ld_T = loss_fn(U_batch, U_pred)\n",
    "    loss_data = tf.stack([Ld_u, Ld_w, Ld_p, Ld_T], axis=0)\n",
    "    loss = (l_dwa[0]*Ld_u + l_dwa[1]*Ld_w + l_dwa[2]*Ld_p + l_dwa[3]*Ld_T)\n",
    "    \n",
    "  grad = tape.gradient(loss, context_builder.trainable_variables + sequence_generator.trainable_variables)\n",
    "  \n",
    "  return loss_data, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33664261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER VARIABLES FOR TRAINING\n",
    "loss_history = []\n",
    "w = tf.constant(0.9, dtype=tf.float32)\n",
    "l_dwa = tf.Variable(tf.ones([4], tf.float32) / 4, trainable=False)\n",
    " \n",
    "best_loss = float('inf')\n",
    "patience = 20\n",
    "decay_rate = 0.8\n",
    "\n",
    "dwa_steps = tf.Variable(16, dtype=tf.int32, trainable=False)\n",
    "acc_steps = tf.Variable(4, dtype=tf.int32, trainable=False)\n",
    "\n",
    "grad_acc = [tf.Variable(tf.zeros_like(v), trainable=False) for v in context_builder.trainable_variables]\n",
    "grad_acc += [tf.Variable(tf.zeros_like(v), trainable=False) for v in sequence_generator.trainable_variables]\n",
    "\n",
    "# WE USE TEACHER FORCING WITH SCHEDULED SAMPLING\n",
    "## The training starts with the model predicting one step ahead. \n",
    "## Gradually we expose the model to longer output sequences up to the full 60\n",
    "rec_prob = tf.Variable(0., trainable=False)\n",
    "cur_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "rec_prob_val = tf.constant(1.)\n",
    "\n",
    "stop_teach = look_fwd*20\n",
    "warmup = 250\n",
    "learning_rate = 1-3\n",
    "min_lr = 1e-4\n",
    "wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TRAINING\n",
    "for epoch in tf.range(1, epochs, dtype=tf.float32):\n",
    "  \n",
    "  rec_prob.assign( tf.minimum(1., epoch / stop_teach) )\n",
    "  \n",
    "  Ldata = tf.zeros([4], tf.float32)\n",
    "  val_loss = tf.zeros([4], tf.float32)\n",
    "    \n",
    "  for step, (x_train, x_dec_train, U_train) in enumerate(data_train):\n",
    "    cur_step.assign(step)\n",
    "    \n",
    "    Ldata_b, grad_data_b = train_step(x_train, x_dec_train, U_train, l_dwa, rec_prob)\n",
    "    Ldata += Ldata_b\n",
    "    \n",
    "    for i in tf.range(len(grad_acc)):\n",
    "      grad_acc[i].assign_add(grad_data_b[i] / tf.cast(acc_steps, tf.float32) )\n",
    "      \n",
    "    loss_history.append(Ldata_b)\n",
    "\n",
    "    # Loss balancing & gradient accumulation\n",
    "    if (cur_step+1) % acc_steps == 0: \n",
    "    \n",
    "      dwa_rollmean = tf.reduce_mean(loss_history[-dwa_steps:-1], axis=0)\n",
    "      l_dwa.assign( tf.nn.softmax(tf.cast(loss_history[-1] / dwa_rollmean, tf.float32)) )\n",
    "      \n",
    "      optimizer.apply_gradients(zip(grad_acc, context_builder.trainable_variables + sequence_generator.trainable_variables))\n",
    "      \n",
    "      for i in tf.range(len(grad_acc)):\n",
    "        grad_acc[i].assign(tf.zeros_like(grad_data_b[i]))\n",
    "\n",
    "  Ldata /= (step+1)\n",
    "  for step_val, (x_val, x_dec_val, U_val) in enumerate(data_val):      \n",
    "    val_loss_b = val_step(x_val, x_dec_val, U_val, rec_prob_val)\n",
    "    val_loss = tf.math.add_n([val_loss, val_loss_b])\n",
    "    \n",
    "  val_loss /= (step_val+1)\n",
    "  val_data_loss = tf.reduce_mean(val_loss[:4])\n",
    "  val_loss = val_loss[-4:]\n",
    "\n",
    "  # Learning rate scheduler\n",
    "  if epoch >= warmup:\n",
    "    if (best_loss - tf.reduce_mean(val_loss)) > 1e-3: \n",
    "      best_loss = tf.reduce_mean(val_loss)\n",
    "      wait = 0\n",
    "    else:\n",
    "      wait += 1\n",
    "      \n",
    "    if wait >= patience:\n",
    "      new_lr = max(learning_rate * decay_rate, min_lr)\n",
    "      if new_lr < learning_rate:\n",
    "        learning_rate = new_lr\n",
    "        optimizer.learning_rate.assign(learning_rate)\n",
    "        \n",
    "      wait = 0\n",
    "   \n",
    "  log1 = f\"{tf.cast(epoch, tf.int32)}. Data:{tf.reduce_mean(Ldata[:4]):.2e}, \"\n",
    "  log2 = f\"MC:{Ldata[4]:.2e}, u:{Ldata[5]:.2e}, w:{Ldata[6]:.2e}, T:{Ldata[7]:.2e}, V Data:{val_data_loss:.2e}\"\n",
    "  \n",
    "  print(log1+log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1725a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to forecast with PI-CRNN given an input sequence\n",
    "def run_forecast(input_rbc, horizon):\n",
    "    input_encoder = ae_encoder.predict(tf.expand_dims(input_rbc, axis=0), verbose=0)\n",
    "    h, c = context_builder(input_encoder, training=False)\n",
    "    x = sequence_generator(input_encoder[:,-1:], h, c, horizon, training=False)\n",
    "    forecast = ae_decoder.predict(x, verbose=0, batch_size=8) \n",
    "    return forecast[0]\n",
    "\n",
    "ensembles = 30\n",
    "horizon = 20\n",
    "\n",
    "forecast_starts = np.linspace(look_back, val_size-1, ensembles).astype('int')\n",
    "\n",
    "forecast_ensemble_crnn = []\n",
    "for s in forecast_starts:\n",
    "    input_rbc_temp = data_val[(s-look_back):s]\n",
    "    forecast_temp = run_forecast(input_rbc_temp, horizon)\n",
    "    forecast_ensemble_crnn.append(forecast_temp)\n",
    "\n",
    "np.save('forecast_ensemble_crnn.npy', forecast_ensemble_crnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb278076",
   "metadata": {},
   "source": [
    "# PI-ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# HELPER FUNCTIONS TO COMPUTE DERIVATIVES\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[1], dtype=tf.float32)]) \n",
    "def DT_tf(var, dt):\n",
    "    ddt1 = (var[...,1:2,:,:,:] - var[...,:1,:,:,:]) / dt\n",
    "    ddt = (var[...,2:,:,:,:] - var[...,:-2,:,:,:]) / (2*dt)\n",
    "    ddt2 = (var[...,-2:-1,:,:,:] - var[...,-1:,:,:,:]) / (-dt)\n",
    "    ddt = tf.concat([ddt1,ddt,ddt2], axis=-4)\n",
    "    return ddt\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[256], dtype=tf.float32)]) \n",
    "def DX_tf(var, dx):\n",
    "    dx = tf.reshape(dx, [1,1,tf.shape(dx)[0],1,1])\n",
    "    ddx1 = var[...,1:2,:,:] - var[...,:1,:,:]\n",
    "    ddx = var[...,2:,:,:] - var[...,:-2,:,:]\n",
    "    ddx2 = var[...,-2:-1,:,:] - var[...,-1:,:,:]\n",
    "    ddx = tf.concat([ddx1, ddx, ddx2], axis=-3)\n",
    "    return ddx / dx \n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[256], dtype=tf.float32)])  \n",
    "def DZ_tf(var, dz):\n",
    "    dz = tf.reshape(dz, [1,1,1,tf.shape(dz)[0],1])\n",
    "    ddz1 = var[...,:,1:2,:] - var[...,:,:1,:]\n",
    "    ddz = var[...,:,2:,:] - var[...,:,:-2,:]\n",
    "    ddz2 = var[...,:,-2:-1,:] - var[...,:,-1:,:]\n",
    "    ddz = tf.concat([ddz1,ddz,ddz2], axis=-2)\n",
    "    return ddz / dz \n",
    "\n",
    "#PHYSICS LOSS WRT MASS, MOMENTUM, ENERGY CONSERVATION\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32)])\n",
    "def loss_ns(U_true, U_pred): \n",
    "    loss_data = tf.reduce_mean(tf.math.square(U_pred-U_true), axis=[0,1,2])  \n",
    "    Ld_u = loss_data[0]\n",
    "    Ld_w = loss_data[1]\n",
    "    Ld_p = loss_data[2]\n",
    "    Ld_T = loss_data[3]\n",
    "\n",
    "    U_pred_x = DX_tf(U_pred, dx)\n",
    "    U_pred_z = DZ_tf(U_pred, dz)\n",
    "    U_pred_t  = DT_tf(U_pred, dt)\n",
    "    U_pred_xx = DX_tf(U_pred_x, dx)\n",
    "    U_pred_zz = DZ_tf(U_pred_z, dz)  \n",
    "\n",
    "    u, w, p, T = tf.split(U_pred, 4, axis=-1)\n",
    "    u_x, w_x, p_x, T_x = tf.split(U_pred_x, 4, axis=-1)\n",
    "    u_z, w_z, p_z, T_z = tf.split(U_pred_z, 4, axis=-1)\n",
    "    u_t, w_t, _, T_t = tf.split(U_pred_t, 4, axis=-1)\n",
    "    u_xx, w_xx, _, T_xx = tf.split(U_pred_xx, 4, axis=-1)\n",
    "    u_zz, w_zz, _, T_zz = tf.split(U_pred_zz, 4, axis=-1)\n",
    "\n",
    "    f_mc = u_x + w_z     \n",
    "    f_u = u_t + u*u_x + w*u_z + p_x - tf.math.sqrt(Pr/Ra)*(u_xx + u_zz)\n",
    "    f_w = w_t + u*w_x + w*w_z + p_z - tf.math.sqrt(Pr/Ra)*(w_xx + w_zz) - T\n",
    "    f_T = T_t + u*T_x + w*T_z - (T_xx + T_zz)/tf.math.sqrt(Pr*Ra)\n",
    "      \n",
    "    L_mc = tf.reduce_mean(tf.math.square(f_mc))\n",
    "    L_u = tf.reduce_mean(tf.math.square(f_u))\n",
    "    L_w = tf.reduce_mean(tf.math.square(f_w))\n",
    "    L_T = tf.reduce_mean(tf.math.square(f_T))\n",
    "\n",
    "    return Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "ae_path_model = 'cae.keras'\n",
    "\n",
    "ae_convESN = build_ae_ESN(nodes=128, kernel_size=3, ae_path_model=ae_path_model, look_back=80)\n",
    "\n",
    "data_train_esn, data_val_esn, x, z = load_esn_data(train_size, val_size, look_back, batch_size, Uf, P, T_h, T_0, ae_path_model)\n",
    "\n",
    "low_dims = 16\n",
    "input_signature_train = [tf.TensorSpec(shape=[batch_size, look_back, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                         tf.TensorSpec(shape=[batch_size, 256, 256, 4], dtype=tf.float32),\n",
    "                         tf.TensorSpec(shape=[8], dtype=tf.float32),\n",
    "                         tf.TensorSpec(shape=[2], dtype=tf.float32)]\n",
    "\n",
    "@tf.function(input_signature=input_signature_train)\n",
    "def train_step(x_batch, U_batch, l_dwa, l_g):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        U_pred = ae_convESN(x_batch, training=True)\n",
    "        Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T = loss_ns(U_batch, U_pred)\n",
    "        ldata = (l_dwa[0]*Ld_u + l_dwa[1]*Ld_w + l_dwa[2]*Ld_p + l_dwa[3]*Ld_T)\n",
    "        lpde = (l_dwa[4]*L_mc + l_dwa[5]*L_u + l_dwa[6]*L_w + l_dwa[7]*L_T)\n",
    "        loss_data_pde = tf.stack([Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T], axis=0)\n",
    "    \n",
    "    grad_data = tape.gradient(ldata, ae_convESN.trainable_variables)\n",
    "    grad_data_flat = tf.concat([tf.reshape(g, [-1]) for g in grad_data if g is not None], axis=0)\n",
    "    grad_data_norm = tf.math.sqrt(tf.reduce_mean(tf.math.square(grad_data_flat)))\n",
    "    \n",
    "    grad_pde = tape.gradient(lpde, ae_convESN.trainable_variables)\n",
    "    grad_pde_flat = tf.concat([tf.reshape(g, [-1]) for g in grad_pde if g is not None], axis=0) \n",
    "    grad_pde_norm = tf.math.sqrt(tf.reduce_mean(tf.math.square(grad_pde_flat)))\n",
    "    \n",
    "    grad = [l_g[0]*g_data + l_g[1]*g_pde for g_data, g_pde in zip(grad_data, grad_pde)]\n",
    "    grad_data_pde_norms = tf.stack([grad_data_norm, grad_pde_norm], axis=0)\n",
    "  \n",
    "    optimizer.apply_gradients(zip(grad, ae_convESN.trainable_variables))\n",
    "    return loss_data_pde, grad_data_pde_norms\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, look_back, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[batch_size, 256, 256, 4], dtype=tf.float32)])\n",
    "def val_step(x_batch, U_batch):\n",
    "    U_pred = ae_convESN(x_batch, training=False)\n",
    "    Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T = loss_ns(U_batch, U_pred)\n",
    "    return tf.stack([Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T], axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "learning_rate = 1e-3\n",
    "best_loss = float('inf')\n",
    "\n",
    "l_g = tf.convert_to_tensor(np.array([1.,1.], dtype=np.float32)) \n",
    "l_dwa = tf.ones([8], tf.float32) / 8\n",
    "grad_norms = tf.zeros([2], tf.float32)\n",
    "loss_history = []\n",
    "dwa_steps = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Ldata_pde = tf.zeros([8], tf.float32)\n",
    "    val_loss = tf.zeros([8], tf.float32)\n",
    "  \n",
    "    for step, (x_batch, U_batch) in enumerate(data_train_esn):\n",
    "        Ldata_pde_b, grad_norms_b = train_step(x_batch, U_batch, l_dwa, l_g)\n",
    "        Ldata_pde += Ldata_pde_b\n",
    "        grad_norms = tf.math.add_n([grad_norms, grad_norms_b])\n",
    "        loss_history.append(Ldata_pde)\n",
    "    \n",
    "    if (step+1) % dwa_steps == 0: \n",
    "        dwa_rollmean = tf.reduce_mean(loss_history[-dwa_steps:-1], axis=0)\n",
    "        l_dwa = tf.nn.softmax(tf.cast(loss_history[-1] / dwa_rollmean, tf.float32))\n",
    "        \n",
    "        l_g_data_hat = tf.reduce_sum(grad_norms)/grad_norms[0]\n",
    "        l_g_pde_hat = tf.reduce_sum(grad_norms)/grad_norms[1] \n",
    "        l_g_hat = tf.stack([l_g_data_hat, l_g_pde_hat])\n",
    "        l_g = 0.9*l_g + 0.1*l_g_hat\n",
    "        grad_norms = tf.zeros([2], tf.float32)     \n",
    "    Ldata_pde /= (step+1)  \n",
    "  \n",
    "    for step, (x_batch, U_batch) in enumerate(data_val_esn):\n",
    "        val_loss += val_step(x_batch, U_batch)\n",
    "    val_loss /= (step+1)\n",
    "  \n",
    "  \n",
    "    val_data_loss = tf.reduce_mean(val_loss[:4])\n",
    "    val_pde_loss = val_loss[-4:] \n",
    "    if (best_loss - tf.reduce_mean(val_pde_loss)) > 1e-3: \n",
    "        best_loss = tf.reduce_mean(val_pde_loss)\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "    \n",
    "    if wait >= 20:\n",
    "        new_lr = max(learning_rate * 0.8, 1e-4)\n",
    "        if new_lr < learning_rate:\n",
    "            learning_rate = new_lr\n",
    "            optimizer.learning_rate.assign(learning_rate)\n",
    "        wait = 0\n",
    "  \n",
    "  \n",
    "    log1 = f\"{epoch+1}. Data: {tf.reduce_mean(Ldata_pde[:4]):.2e}, MC: {Ldata_pde[4]:.2e}, u: {Ldata_pde[5]:.2e}, \"\n",
    "    log2 = f\"w: {Ldata_pde[6]:.2e}, T: {Ldata_pde[7]:.2e}, Val Data: {val_data_loss:.2e}, \"\n",
    "    log3 = f\"Val MC: {val_pde_loss[0]:.2e}, Val u: {val_pde_loss[1]:.2e}, Val w: {val_pde_loss[2]:.2e}, Val T: {val_pde_loss[3]:.2e}\"\n",
    "    print(log1+log2+log3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690197ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((data_train, data_val), axis=0)\n",
    "\n",
    "@tf.function\n",
    "def predict_step(input_seq):\n",
    "    input_temp = ae_encoder(input_seq, training=False)\n",
    "    return ae_convESN(input_temp, training=False)\n",
    "\n",
    "forecast_ensemble_pi_esn = []\n",
    "forecast_starts = np.linspace(look_back, val_size-1, ensembles).astype('int')\n",
    "for s in forecast_starts:\n",
    "\n",
    "    ensemble_member = data[s-look_back:s]\n",
    "    for i in range(look_back, look_back+horizon):\n",
    "        output_temp = predict_step(tf.expand_dims(ensemble_member[-look_back:], axis=0))\n",
    "        ensemble_member = tf.concat([ensemble_member, output_temp], axis=0)\n",
    "\n",
    "    forecast_ensemble_pi_esn.append( np.array(ensemble_member[look_back:]) )\n",
    "\n",
    "np.save('forecast_ensemble_pi_esn.npy', forecast_ensemble_pi_esn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea6810",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_ens = 30\n",
    "def fit_arima(time_series, val_size=378, n_ens=n_ens):\n",
    "\n",
    "    forecast = np.zeros((n_ens, val_size, 4))\n",
    "    for k in range(4):\n",
    "        ts = time_series[:,k]\n",
    "\n",
    "        # Near constant fallback, ARIMA will fail \n",
    "        if np.std(ts) < 1e-6:\n",
    "            forecast[:,:,k] = ts[-1]\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            model = ARIMA(ts, order=(10, 0, 0), trend=\"t\")\n",
    "            model_fit = model.fit()\n",
    "            sims = model_fit.simulate(nsimulations=val_size, repetitions=n_ens, anchor=\"end\")\n",
    "            forecast[:,:,k] = sims.T\n",
    "\n",
    "        except (ValueError, np.linalg.LinAlgError):\n",
    "            # Fallback on failure\n",
    "            forecast[:,:,k] = ts[-1]\n",
    "\n",
    "    return forecast\n",
    "\n",
    "data_arima_train = data_train.reshape((train_size, 256*256, 4))\n",
    "results = Parallel(n_jobs=-1)(delayed(fit_arima)(data_arima_train[:,i,:], val_size) for i in range(256*256))\n",
    "\n",
    "results = np.asarray(results) # (n_cells, n_ens, val_size, 4)\n",
    "results_arima = results.reshape(256, 256, n_ens, val_size, 4)\n",
    "\n",
    "results_arima = np.moveaxis(results_arima, [2, 3], [0, 1]) # (n_ens, val_size, 256, 256, 4)\n",
    "\n",
    "np.save('forecast_ensemble_arima.npy', results_arima)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
